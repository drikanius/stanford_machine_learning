{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with One Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Consider the problem of predicting how well a student does in her second year of college/university, given how well she did in her first year.\n",
    "\n",
    "Specifically, let x be equal to the number of \"A\" grades (including A-. A and A+ grades) that a student receives in their first year of college (freshmen year). We would like to predict the value of y, which we define as the number of \"A\" grades they get in their second year (sophomore year).\n",
    "\n",
    "Here each row is one training example. Recall that in linear regression, our hypothesis is $ h_θ = θ_0 + θ_1x $, and we use _m_ to denote the number of training examples.\n",
    "\n",
    "| x | y |\n",
    "| - |:-:| \n",
    "| 3 | 2 |\n",
    "| 1 | 2 | \n",
    "| 0 | 1 |\n",
    "| 4 | 3 | \n",
    "\n",
    "\n",
    "For the training set given above (note that this training set may also be referenced in other questions in this quiz), what is the value of mm? In the box below, please enter your answer (which should be a number between 0 and 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "For this question, assume that we are\n",
    "\n",
    "using the training set from Q1. Recall our definition of the\n",
    "\n",
    "cost function was $ J(θ_0, θ_1) = \\frac{1}{2m}\\sum_{i=1}^m (h_θ(x^{(i)}) - y^{(i)})^2   $.\n",
    "\n",
    "What is $ J(0, 1) $? In the box below,\n",
    "\n",
    "please enter your answer (Simplify fractions to decimals when entering answer, and '.' as the decimal delimiter e.g., 1.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "$ h_0 = θ_0 + θ_1x = x $;\n",
    "$ \\frac{1}{2*4}\\ * 4 = 0.5 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "\n",
    "Suppose we set $ θ_0 = -1, θ_1 = 0.5$. What is $ h_θ(4) $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:** $ h_0(4) = -1 + 0.5 * 4 = 1 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "\n",
    "Let _f_ be some function so that $ f(θ_0, θ_1) $ outputs a number. For this problem, _f_ is some arbitrary/unknown smooth function (not necessarily the cost function of linear regression, so _f_ may have local optima).\n",
    "\n",
    "Suppose we use gradient descent to try to minimize $ f(θ_0, θ_1) $ as a function of $ θ_0 $ and  $ θ_1 $. Which of the following statements are true? (Check all that apply.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:\n",
    "\n",
    "**(True)** If the first few iterations of gradient descent cause $ f(θ_0, θ_1) $ to _increase_ rather than decrease, then the most likely cause is that we have set the learning rate \\alphaα to too large a value.\n",
    "\n",
    "**(False)** Setting the learning rate $ α $ to be very small is not harmful, and can only speed up the convergence of gradient descent.\n",
    "\n",
    "**(True)** If $ θ_0 $ and $ θ_1 $ are initialized at the global minimum, then one iteration will not change their values.\n",
    "\n",
    "**(False)** No matter how $ θ_0 $ and $ θ_1 $ are initialized, so long as $α$ is sufficiently small, we can safely expect gradient descent to converge to the same solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "\n",
    "Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some $ θ_0 $, $ θ_1 $ such that $ J(θ_0, θ_1)=0 $.\n",
    "\n",
    "Which of the statements below must then be true? (Check all that apply.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:\n",
    "\n",
    "**(False)** We can perfectly predict the value of yy even for new examples that we have not yet seen. (e.g., we can perfectly predict prices of even new houses that we have not yet seen.)\n",
    "\n",
    "**(False)** This is not possible: By the definition of $J(θ_0, θ_1)$, it is not possible for there to exist $ θ_0 $ and $ θ_1 $ so that $J(θ_0, θ_1) = 0$.\n",
    "\n",
    "**(True)** For these values of $ θ_0 $ and $ θ_1 $ that satisfy $J(θ_0, θ_1) = 0$ we have that $ h_θ(x^{(i)}) = y^{(i)} $ for every training example $ (x^{(i)}, y^{(i)}) $\n",
    "\n",
    "**(False)** For this to be true, we must have $ θ_0 = 0 $ and  $ θ_1 = 0 $ so that $ h_θ(x) = 0 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
